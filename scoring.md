# Scoring

We use [F1 score](https://en.wikipedia.org/wiki/F1_score) to evaluate the performance of our model. We will have two labels, `True` and `False` for each word pair. Using the gold labels and labels generated by the models, we will be able to calculate the recall and precision for the `True` label, and thus calculate the $F_1$ score.

To evaluate, run `python evaluate.py --gold [goldfile] --pred [predfile]`.